
---
title: "03-Analysis"
author: "Danielle Ethier"
date: "2022-07-26"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Load libraries

```{r library}

library(tidyverse)
#install.packages("INLA", repos=c(getOption("repos"), 
#        INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE)
library(INLA)
library(inlabru)
library(spdep)
library(sf)
library(VGAM)
library(pROC)
library(DMwR2)

#devtools::install_github("timcdlucas/INLAutils") #tools for testing spatail models
library(INLAutils)

#remotes::install_github("inbo/inlatools")
library(inlatools)

library(maps)
library(ggplot2)
library(terra)
library(tidyterra) # raster plotting
#library(scales)

#make need to install the test version to make this work. 
library("devtools")
#install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE)
library(INLA)
library(inlabru)
library(fmesher)
library(sp)
library(reshape)
library(MatrixModels)

```

#Load data and set directories

Note: this is a zero-filled and range reduced data table

```{r data}

dat<-read.csv("Output/CWMP_MaxOcc.csv") #data that does not consider detection limit
#dat<-read.csv("Output/CWMP_MaxOccV2.csv") #data that considers detection limit
out.dir<-"Output/"

```

#Make spatial grid

```{r spatial grid}

Grid <- st_read(dsn="C:/Users/dethier/Documents/ethier-scripts/CWMP_Frogs/Data", layer="greatlakes_subbasins")
grid_key <- unique(dat[, c("basin")])
row.names(grid_key) <- NULL

nb1 <- poly2nb(Grid, row.names=Grid$data); nb1

is.symmetric.nb(nb1, verbose = FALSE, force = TRUE)
nb2INLA("nb1.graph", nb1)
nb1.adj <- paste(getwd(),"/nb1.graph", sep="")
g1 <- inla.read.graph("nb1.graph")

```

#Output tables

```{r output}

##-----------------------------------------------------------
##Create output tables before entering the loop

#Annual index of abundance output table
d3<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 6, byrow = FALSE, dimnames = NULL))
names(d3) <- c("basin", "year", "taxa_code", "abund", "abund_lci", "abund_cui")
write.table(d3, file = paste(out.dir, "AnnualIndex.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

#Tau Total
tau_prov<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 8, byrow = FALSE, dimnames = NULL))
names(tau_prov) <- c("StateProvince","med_tau", "lcl_tau", "ucl_tau", "iw_tau", "n", "cum_prob", "taxa_code")
write.table(tau_prov, file = paste(out.dir, "Tau_All.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

#Alpha Total
alpha_prov<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 7, byrow = FALSE, dimnames = NULL))
names(alpha_prov) <- c("StateProvince","med_alpha", "lcl_alpha", "ucl_alpha", "iw_alpha", "n", "taxa_code")
write.table(alpha_prov, file = paste(out.dir, "Alpha_All.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

#Posterior Summary
post_sum<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 13, byrow = FALSE, dimnames = NULL))
names(post_sum) <- c("alpha_i", "alph", "alph_ll", "alph_ul", "alph_iw", "tau", "tau_ll", "tau_ul", "tau_iw", "tau_sig", "cum_prob", "basin", "taxa_code")
write.table(post_sum, file = paste(out.dir, "PosteriorSummary.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

#AUC summary 
AUCtable<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 3, byrow = FALSE, dimnames = NULL))
names(AUCtable) <- c("model", "AUC", "t.model")
write.table(AUCtable, file = paste(out.dir, "AUC_Table.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")



```

#Data prep and exploration

```{r data prep}
#scale covariates, but not scale.lake since this is already scaled and not the % covariates
#dat <-dat %>% mutate_at(c("Roadkm", "SolReacP", "nitrateN", "ammoniaN", "cond"), ~(scale(.) %>% as.vector))

#remove outliers
dat.out<-dat %>% select(Roadkm, SolReacP, nitrateN, ammoniaN, cond) %>% 
    mutate(across(
    where(is.numeric),
    ~ ifelse(
      abs(as.numeric(scale(.x))) > 3,
      NA, 
      .x
    )
  ))

dat<-dat %>% select(-Roadkm, -SolReacP, -nitrateN, -ammoniaN, -cond) 
dat<-cbind(dat, dat.out)

#perp other variables
sp.list<-unique(dat$taxa_code)
max.yr<-max(dat$year)

#standardize year to 2023, prepare index varaibles 
#where i = grid cell (basin), k = site, t = year
dat <- dat %>% mutate(std_yr = year - max.yr)
dat$class<-as.factor(dat$class)
dat$basin<-as.factor(dat$basin)
dat$taxa_code<-as.factor(dat$taxa_code)
dat$kappa_k <- as.integer(factor(dat$site_id)) #index for the random site (wetland) effect
dat$tau_i <- dat$alpha_i <- as.integer(factor(dat$basin)) #index for each basin intercept and slope

#Specify model with year-basin effects so that we can predict the annual index value for each basin
dat$gamma_ij <- paste0(dat$alpha_i, "-", dat$year)
dat$yearfac = as.factor(dat$year)

#species analysis loop
for(m in 1:length(sp.list)) {

 #m<-1 #for testing each species

  sp.data <-NULL 
  sp.data <- filter(dat, taxa_code == sp.list[m]) %>%
      droplevels()
  sp<-sp.list[m] 
  
print(paste("Currently analyzing species ", m, "/", sp.list[m], sep = "")) 

grid_key<-NULL
grid_key <- unique(sp.data[, c("basin", "alpha_i")])
grid_key$StateProvince<-"All"
row.names(grid_key) <- NULL

formula<-maxocc ~ std_yr + scale.lake + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm 

#formula<-maxocc ~ std_yr + scale.lake + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond
sp.dat<-sp.data %>% na.omit()
f0 <- glm(formula, data = sp.dat, family = "binomial")
summary(f0)

glm.predictions <- f0$fitted.values
glm.roc <- roc(response = sp.dat$maxocc, predictor = glm.predictions, auc = TRUE)

f1 <- maxocc ~ -1 + 
  # cell ICAR random intercepts
  f(alpha_i, model="besag", graph=g1, constr=FALSE, scale.model=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) +
  # cell ICAR random year slopes
  f(tau_i, std_yr, model="besag", graph=g1, constr=FALSE, scale.model=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) +
  # random site intercepts
  f(kappa_k, model="iid", constr=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01))))+
 # basin-year effect
  f(gamma_ij, model="iid", constr=TRUE, 
   hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) 

out1<- inla(f1, family="binomial", data=sp.data,
            control.compute=list(cpo=T, dic=TRUE, waic=TRUE, return.marginals.predictor=TRUE, config=TRUE),
            control.inla=list(strategy="adaptive",
                              int.strategy="auto"),
           control.predictor = list(compute=TRUE, link=1),
           control.family=list(link='logit'),
            num.threads=3,
            verbose=T)

#plot observed versus fitted values
#ggplot_inla_residuals(out1, sp.data$maxocc)

#plot model outputs
#p2 <- autoplot(out1)
#cowplot::plot_grid(plotlist = p2)

# INLA-SPDE ROC
predictions <- out1$summary.fitted.values$mean[1:length(sp.data$maxocc)]
roc.data <- roc(response = sp.data$maxocc, predictor = predictions, auc = TRUE)

#sp.data$pred <- out1$summary.fitted.values$mean[1:length(sp.data$maxocc)]
#plot(sp.data$pred, sp.data$maxocc)

f2 <- maxocc ~ -1 + 
  # cell ICAR random intercepts
  f(alpha_i, model="besag", graph=g1, constr=FALSE, scale.model=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) +
  # cell ICAR random year slopes
  f(tau_i, std_yr, model="besag", graph=g1, constr=FALSE, scale.model=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) +
  # random site intercepts
  f(kappa_k, model="iid", constr=TRUE,
    hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01))))+
 # basin-year effect
  f(gamma_ij, model="iid", constr=TRUE, 
   hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01)))) + 
  scale.lake + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm #+ SolReacP + nitrateN + ammoniaN + cond

out2<- inla(f2, family="binomial", data=sp.data,
            control.compute=list(cpo=T, dic=TRUE, waic=TRUE, return.marginals.predictor=TRUE, config=TRUE),
            control.inla=list(strategy="adaptive",
                              int.strategy="auto"),
           control.predictor = list(compute=TRUE, link=1),
           control.family=list(link='logit'),
            num.threads=3,
            verbose=T)

out2$summary.fix

#par(mfrow=c(3,4))
#plot(out2$marginals.fixed[[1]], ty = "l", xlab = expression(beta[scale.lake]), ylab = "Density") 
#plot(out2$marginals.fixed[[2]], ty = "l", xlab = expression(beta[per_forest]), ylab = "Density") 
#plot(out2$marginals.fixed[[3]], ty = "l", xlab = expression(beta[per_wetlg]), ylab = "Density") 
#plot(out2$marginals.fixed[[4]], ty = "l", xlab = expression(beta[per_wetsm]), ylab = "Density") 
#plot(out2$marginals.fixed[[5]], ty = "l", xlab = expression(beta[per_agri]), ylab = "Density") 
#plot(out2$marginals.fixed[[6]], ty = "l", xlab = expression(beta[per_urban]), ylab = "Density") 
#plot(out2$marginals.fixed[[7]], ty = "l", xlab = expression(beta[per_Roadkm]), ylab = "Density") 
#plot(out2$marginals.fixed[[8]], ty = "l", xlab = expression(beta[per_SolReacP]), ylab = "Density") 
#plot(out2$marginals.fixed[[9]], ty = "l", xlab = expression(beta[per_nitrateN]), ylab = "Density") 
#plot(out2$marginals.fixed[10], ty = "l", xlab = expression(beta[per_ammoniaN]), ylab = "Density") 
#plot(out2$marginals.fixed[[11]], ty = "l", xlab = expression(beta[cond]), ylab = "Density") 

# INLA-SPDE ROC all covariates
prediction2 <- out2$summary.fitted.values$mean[1:length(sp.data$maxocc)]
roc.data2 <- roc(response = sp.data$maxocc, predictor = prediction2, auc = TRUE)

model<-c("glm cov", "spatail", "spatail cov")
AUC<-c(as.numeric(glm.roc[["auc"]]), as.numeric(roc.data[["auc"]]), as.numeric(roc.data2[["auc"]]))
t.model<-data.frame(model, AUC)
t.model$species<-sp.list[m]

#print AUC comparison 
write.table(t.model, paste(out.dir, "AUC_Table.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

##Compile results
#Fixed
fixed.out<-out2$summary.fixed[,c("mean", "sd", "0.025quant", "0.975quant")]
fixed.out<-signif(fixed.out, digits = 4)
fixed.out$Species <- sp.list[m]
names(fixed.out)[1:5] <- c("mean", "SD", "0.025quant", "0.975quant", "Speices")
  
write.table(fixed.out, paste(out.dir, "Fixed_Summary.csv"), row.names = TRUE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#Random spatial
random.out<-out2$summary.hyperpar[,c("mean", "sd", "0.025quant", "0.975quant")]
random.out<-signif(random.out, digits = 4)
random.out$Species <- sp.list[m]
names(random.out)[1:5] <- c("mean", "SD", "0.025quant", "0.975quant", "Speices")

write.table(random.out, paste(out.dir, "Random_Summary.csv"), row.names = TRUE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

##Remove cells with no routes
cells_with_counts <- unique(sp.data$alpha_i[which(!is.na(sp.data$maxocc))])

# get alpha summaries
alph <- exp(out2$summary.random$alpha_i$`0.5quant`[cells_with_counts])/ (1+ exp(out2$summary.random$alpha_i$`0.5quant`[cells_with_counts]))
alph_ll <- exp(out2$summary.random$alpha_i$`0.025quant`[cells_with_counts])/(1+exp(out2$summary.random$alpha_i$`0.025quant`[cells_with_counts]))
alph_ul <- exp(out2$summary.random$alpha_i$`0.975quant`[cells_with_counts])/ (1+ exp(out2$summary.random$alpha_i$`0.975quant`[cells_with_counts]))
alph_iw <- alph_ul - alph_ll

# get tau summaries
tau <- (exp(out2$summary.random$tau_i$`0.5quant`[cells_with_counts])- 1) * 100
tau_ll <- (exp(out2$summary.random$tau_i$`0.025quant`[cells_with_counts]) - 1) * 100
tau_ul <- (exp(out2$summary.random$tau_i$`0.975quant`[cells_with_counts]) - 1) * 100
tau_iw <- tau_ul - tau_ll


##-----------------------------------------------------------
#time series plots per cell
#calculate cell level index of abundance

#create a loop to get abundance index output per cell-year

for(k in 1:length(cells_with_counts)) {

#k<-1 #for testing each cell
   
  cell1 <-NULL 
  cell1 <- cells_with_counts[k]
  
#need to back assign the factor cell1 to its original grid_id
cell_id<-sp.data %>% ungroup() %>% dplyr::select(basin, alpha_i) %>% distinct()
grid1<- as.character(cell_id[k,"basin"])
 
#median 
   d0 <- out2$summary.random$alpha_i$`0.5quant`[cell1]
   d1 <- out2$summary.random$tau_i$`0.5quant`[cell1]
   d2 <- data.frame(
   styear=as.numeric(gsub(paste0(cell1,"-"), "",
                        grep(paste0("\\b",cell1,"-"),
                             out2$summary.random$gamma_ij$ID,
                                  value=TRUE)))- max.yr, gamma_ij=
     out2$summary.random$gamma_ij$`0.5quant`[grep(
       paste0("\\b",cell1,"-"), out2$summary.random$gamma_ij$ID)]) %>%
     arrange(styear)
   d2$x0 <- d0
   d2$x1 <- d2$styear*d1
   d2$abund <- exp(d2$x0 + d2$x1 + d2$gamma_ij)/(1+exp(d2$x0 + d2$x1 + d2$gamma_ij)) 
   d2$cell<-cell1
   d2<-merge(d2, grid_key, by.x="cell", by.y="alpha_i")
   d2$taxa_code<-sp
   
   d3<-d2 %>% select(basin, taxa_code, styear, abund) %>% mutate(year=styear+2023) %>% select(-styear)
 
#lci     
   l0 <- out2$summary.random$alpha_i$`0.025quant`[cell1]
   l1 <- out2$summary.random$tau_i$`0.025quant`[cell1]
   l2 <- data.frame(
   styear=as.numeric(gsub(paste0(cell1,"-"), "",
                        grep(paste0("\\b",cell1,"-"),
                             out2$summary.random$gamma_ij$ID,
                                  value=TRUE)))- max.yr, gamma_ij=
     out2$summary.random$gamma_ij$`0.025quant`[grep(
       paste0("\\b",cell1,"-"), out2$summary.random$gamma_ij$ID)]) %>%
     arrange(styear)
   l2$x0 <- l0
   l2$x1 <- l2$styear*l1
   l2$abund_lci <- exp(l2$x0 + l2$x1 + l2$gamma_ij)/(1+exp(l2$x0 + l2$x1 + l2$gamma_ij))
   l2$cell<-cell1
   l2<-merge(l2, grid_key, by.x="cell", by.y="alpha_i")
   
  l3<-l2 %>% select(basin, styear, abund_lci) %>% mutate(year=styear+2023) %>% select(-styear) 

#uci  
 u0 <- out2$summary.random$alpha_i$`0.975quant`[cell1]
   u1 <- out2$summary.random$tau_i$`0.975quant`[cell1]
   u2 <- data.frame(
   styear=as.numeric(gsub(paste0(cell1,"-"), "",
                        grep(paste0("\\b",cell1,"-"),
                             out2$summary.random$gamma_ij$ID,
                                  value=TRUE)))- max.yr, gamma_ij=
     out2$summary.random$gamma_ij$`0.975quant`[grep(
       paste0("\\b",cell1,"-"), out2$summary.random$gamma_ij$ID)]) %>%
     arrange(styear)
   u2$x0 <- u0
   u2$x1 <- u2$styear*u1
   u2$abund_uci <- exp(u2$x0 + u2$x1 + u2$gamma_ij)/(1+exp(u2$x0 + u2$x1 + u2$gamma_ij))
   u2$cell<-cell1
   u2<-merge(u2, grid_key, by.x="cell", by.y="alpha_i")
   
u3<-u2 %>% select(basin, styear, abund_uci) %>% mutate(year=styear+2023) %>% select(-styear)   

d3<-merge(d3, l3, by=c("basin", "year"))
d3<-merge(d3, u3, by=c("basin", "year"))

write.table(d3, paste(out.dir, "AnnualIndex.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)
   
   } #end cell specific loop


##-----------------------------------------------------------
#Explore posterior samples 

grid2<-grid_key %>% filter(alpha_i==cells_with_counts)

posterior_ss <- 1000 # change as appropriate
samp1 <- inla.posterior.sample(posterior_ss, out2, num.threads=3)
par_names <- as.character(attr(samp1[[1]]$latent, "dimnames")[[1]])
post1 <- as.data.frame(sapply(samp1, function(x) x$latent))
post1$par_names <- par_names
 
# tau samples
tau_samps1 <- post1[grep("tau_i", post1$par_names), ]
row.names(tau_samps1) <- NULL
tau_samps1 <- tau_samps1[cells_with_counts, 1:posterior_ss]
tau_samps1 <- (exp(tau_samps1) - 1) * 100
tau_samps2 <- cbind(grid2, tau_samps1)
row.names(tau_samps2) <- NULL
val_names <- grep("V", names(tau_samps2))

#calculate cumulative frequency trend is <0
cum_prob<-tau_samps2 %>% select(-basin, -StateProvince) %>% melt(id.vars="alpha_i")
basin.list<-unique(cum_prob$alpha_i)

#species analysis loop
for(p in 1:length(basin.list)) {

  bl<-cum_prob %>% filter(alpha_i == basin.list[p])
  basin.list[p]<-ecdf(bl$value)(0)

}#end basin loop

#tau_prov
#not weighted for basin area. New calculation provided below using PosteriorSummary outputs. 
tau_prov <- tau_samps2 %>%
  ungroup() %>%  #this seems to be needed before the select function or it won't work
  dplyr::select(StateProvince, val_names) %>%
  mutate(StateProvince=factor(StateProvince)) %>%
  gather(key=key, val=val, -StateProvince) %>%
  dplyr::select(-key) %>%
  group_by(StateProvince) %>%
  summarise(med_tau=median(val), lcl_tau=quantile(val, probs=0.025),
            ucl_tau=quantile(val, probs=0.975), iw_tau=ucl_tau-lcl_tau,
            n=n()/posterior_ss, cum_prob=ecdf(val)(0)); head(tau_prov)
tau_prov$taxa_code <- sp.list[m]


write.table(tau_prov, paste(out.dir, "Tau_All.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

# alpha samples
alpha_samps1 <- post1[grep("alpha_i", post1$par_names), ]
row.names(alpha_samps1) <- NULL
alpha_samps1 <- alpha_samps1[cells_with_counts, 1:posterior_ss]
alpha_samps1 <- exp(alpha_samps1)/(1+exp(alpha_samps1)) 
alpha_samps2 <- cbind(grid2, alpha_samps1)
row.names(alpha_samps2) <- NULL
val_names <- grep("V", names(alpha_samps2))

#alpha_prov
#not weighted for basin area. New calculation provided below using PosteriorSummary outputs.
alpha_prov <- alpha_samps2 %>%
  ungroup() %>%  #this seems to be needed before the select function or it won't work
  dplyr::select(StateProvince, val_names) %>%
  mutate(StateProvince=factor(StateProvince)) %>%
  gather(key=key, val=val, -StateProvince) %>%
  dplyr::select(-key) %>%
  group_by(StateProvince) %>%
  summarise(med_alpha=median(val), lcl_alpha=quantile(val, probs=0.025),
            ucl_alpha=quantile(val, probs=0.975), iw_alpha=ucl_alpha-lcl_alpha,
            n=n()/posterior_ss); head(alpha_prov)
alpha_prov$taxa_code <- sp.list[m]
  
write.table(alpha_prov, paste(out.dir, "Alpha_All.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

##-----------------------------------------------------------
#Collect posterior summaries into one data frame

post_sum<-NULL
post_sum <- data.frame(alpha_i=cells_with_counts,
                       alph, alph_ll, alph_ul, alph_iw,
                       #eps, eps_ll, eps_ul, eps_iw, eps_sig=NA,
                       tau, tau_ll, tau_ul, tau_iw, tau_sig=NA, basin.list)
post_sum$tau_sig <- ifelse((post_sum$tau_ll < 1 & post_sum$tau_ul > 1),
                           post_sum$tau_sig <- 0,
                           post_sum$tau_sig <- post_sum$tau)


#need to back assign the factor alpha_id to its original value
id_grid<-sp.data %>% ungroup() %>% dplyr::select(alpha_i, basin) %>% distinct()
post_sum<-merge(post_sum, cell_id, by="alpha_i")
post_sum$taxa_code<-sp


write.table(post_sum, paste(out.dir, "PosteriorSummary.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

##-----------------------------------------------------------
#Model prediction/ interpretation
#Develop linear combinations

#write species data
write.table(sp.data, paste(out.dir, sp.list[m],".csv", sep=""), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#lake level
mydata<-data.frame(per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_wetlg, na.rm=TRUE), 
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE),
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE),
                  scale.lake=seq(from=-1.69377, to=1.96665, length=25) 
                   )

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}


mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"LakeLevelEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#pcntforest
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=seq(from = 0, to = 100, length = 25), 
                  per_wetlg=mean(dat$per_wetlg, na.rm=TRUE), 
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE)
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE))
)


Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)

Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"PcntForestEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#pcnwetlg
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=seq(from = 0, to = 100, length = 25),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE)
                # SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                #  nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                #  ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                #  cond=mean(dat$cond, na.rm=TRUE))
)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"PcntWetlandEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#PerWetlandLocal
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=seq(from = 0, to = 100, length = 25), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE)
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE))
)

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)

Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"PcntWetlandLocalEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#PerAgri
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=seq(from = 0, to = 100, length = 25),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE)
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE))
)
                  
Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"PcntAgriEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#PerUrban
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=seq(from = 0, to = 100, length = 25),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE)
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE))
)

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)

Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"PcntUrbanEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#Road
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=seq(from = 0, to = 6, length=25)
                  #SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  #nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  #ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  #cond=mean(dat$cond, na.rm=TRUE))
)
                  
Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + scale.lake, data=mydata)

#Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"RoadEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

} #end species loop


##ADD BACK TO LOOP IF INCLUDING ALL COVARAITES
#SolReacP
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE),
                  SolReacP=seq(from = -0.004, to = 0.134, length = 25),
                  nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  cond=mean(dat$cond, na.rm=TRUE))

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"SolReacPEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#nitrateN
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE),
                  SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  nitrateN=seq(from = 0.026, to = 2.74, length = 25),
                  ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  cond=mean(dat$cond, na.rm=TRUE))

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"nitrateNEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#ammoniaN
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE),
                  SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  ammoniaN=seq(from=0, to=0.148, length = 25),
                  cond=mean(dat$cond, na.rm=TRUE))

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"AmmoniaNEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

#cond
mydata<-NULL
mydata<-data.frame(scale.lake=mean(dat$scale.lake, na.rm=TRUE), 
                  per_forest=mean(dat$per_forest, na.rm=TRUE), 
                  per_wetlg=mean(dat$per_forest, na.rm=TRUE),
                  per_wetsm=mean(dat$per_wetsm, na.rm=TRUE), 
                  per_agri=mean(dat$per_agri, na.rm=TRUE),
                  per_urban=mean(dat$per_urban, na.rm=TRUE),
                  Roadkm=mean(dat$Roadkm, na.rm=TRUE),
                  SolReacP=mean(dat$SolReacP, na.rm=TRUE),
                  nitrateN=mean(dat$nitrateN, na.rm=TRUE),
                  ammoniaN=mean(dat$ammoniaN, na.rm=TRUE),
                  cond=seq(from=28, to=974, length = 25))

Xmat<-model.matrix(~ -1 + per_forest + per_wetlg + per_wetsm + per_agri + per_urban + Roadkm + SolReacP + nitrateN + ammoniaN + cond + scale.lake, data=mydata)
Xmat<-as.data.frame(Xmat)

lcb<-inla.make.lincombs(Xmat)

lcb_lake<- inla(f2, family="binomial", data=sp.data,
                lincomb=lcb, 
                control.predictor=list(compute=TRUE, link=1, quantiles=c(0.025, 0.975)), verbose=TRUE)

#get the predicted values
Pred.marg<-lcb_lake[["marginals.lincomb.derived"]]

# now we need to back transform the values

#create transformation function
MyBack<-function(x) {exp(x)/(1+exp(x))}

mydata$mu<-unlist(lapply(Pred.marg, function(x) inla.emarginal(MyBack, x)))
mydata$LCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.025), inla.tmarginal(MyBack,x))))
mydata$UCI<-unlist(lapply(Pred.marg, function(x) inla.qmarginal(c(0.975), inla.tmarginal(MyBack,x))))

write.table(mydata, paste(out.dir, sp.list[m],"CondEffect.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = TRUE)

} #end species loop

```

#Summarize results using outputs

```{r plot index}

out.dir<-"Output/"

index<-read.csv("Output/ AnnualIndex.csv")
index<-index %>% na.omit()
index$basin<-as.factor(index$basin)
index$year<-as.integer(index$year)

ggplot(index, aes(x=year, y=abund, color=basin))+
  geom_point()+
  geom_errorbar(aes(ymin=abund_lci, ymax=abund_cui))+
  geom_smooth(method=loess, fill=NA, span=0.5)+
 # geom_smooth(method=lm, fill=NA)+
  facet_wrap(taxa_code~., scales="free")+
  #facet_wrap(taxa_code~.,)+
  #scale_y_continuous(trans='log10') + annotation_logticks()+
  xlab("Year")+ylab("Index of Occupancy")+
  theme_classic()
 # scale_color_grey(start = 0.8, end = 0.2) 


#plot raw mean abundance and predictor values. 
index.raw<-dat %>% dplyr::group_by(taxa_code, basin, year) %>% dplyr::summarize(index.raw=mean(maxocc))

index<-left_join(index, index.raw, by=c("basin", "year", "taxa_code"))

ggplot(index, aes(x=index.raw, y=abund, color=basin))+
  geom_point()+
  geom_smooth(method=lm, fill=NA)+
  facet_wrap(taxa_code~., scales="free")+
  theme_classic()

#determine how many trends are significant within a given basin
sig<-read.csv("OUtput/ PosteriorSummary.csv")
sig<-sig %>% mutate(sigpos=ifelse(tau_sig>0, 1, 0))
sig<-sig %>% mutate(signeg=ifelse (tau_sig<0, 1, 0))
sig<-sig %>% group_by(basin) %>% dplyr::summarize(nsp=n_distinct(taxa_code), possig=sum(sigpos), negsig=sum(signeg))

#assess covariate
cov<-read.csv("Output/CovariatePlot.csv")
cov<-cov %>% mutate(sigpos = ifelse(LCI>0 & UCI>0, 1, 0), signeg=ifelse(LCI<0 & UCI<0, 1, 0))
cov_sum<-cov %>% group_by(Covariate) %>% dplyr::summarize(nsigpos=sum(sigpos), nsigneg=sum(signeg))


```

#Plot covariate values on predicted values

```{r plot predict}

library(cowplot)

out.dir<-"Output/"
index<-na.omit(index)
sp.list<-unique(index$taxa_code)

for(m in 1:length(sp.list)) {

  sp.dat<-read.csv(paste("Output/", " ",sp.list[m], " .csv", sep=""))
  
agri.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntAgriEffect.csv", sep=""))
agri.plot<-ggplot()+
       geom_ribbon(data=agri.dat, aes(x= pcntag, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=agri.dat, aes(y=mu, x=pcntag))+
    geom_point(data=sp.dat, aes(y=maxocc, x=pcntag))+
    theme_classic()+
    xlab("Percent agriculture")+
    ylab("Max Count")

dev.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntUrbanEffect.csv", sep=""))
dev.plot<-ggplot()+
       geom_ribbon(data=dev.dat, aes(x= pcntdev, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=dev.dat, aes(y=mu, x=pcntdev))+
    geom_point(data=sp.dat, aes(y=maxocc, x=pcntdev))+
    theme_classic()+
    xlab("Percent developed")+
    ylab("Max Count")
  
wet.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntWetlandLocalEffect.csv", sep=""))
wet.plot<-ggplot()+
       geom_ribbon(data=wet.dat, aes(x= PerWetland, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=wet.dat, aes(y=mu, x=PerWetland))+
    geom_point(data=sp.dat, aes(y=maxocc, x=PerWetland))+
    theme_classic()+
    xlab("Percent wetland local")+
    ylab("Max Count")

wetlg.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntWetland=Effect.csv", sep=""))
wet.plot<-ggplot()+
       geom_ribbon(data=wetlg.dat, aes(x= PerWetland, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=wet.dat, aes(y=mu, x=PerWetland))+
    geom_point(data=sp.dat, aes(y=maxocc, x=PerWetland))+
    theme_classic()+
    xlab("Percent wetland")+
    ylab("Max Count")

lake.dat<-read.csv(paste("Output/", " ",sp.list[m], " LakeLevelEffect.csv", sep=""))
lake.plot<-ggplot()+
       geom_ribbon(data=lake.dat, aes(x= lakelevel, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=lake.dat, aes(y=mu, x=lakelevel))+
    geom_point(data=sp.dat, aes(y=maxocc, x=lakelevel))+
    theme_classic()+
    xlab("Variation in Lake level")+
    ylab("Max Count")
    
road.dat<-read.csv(paste("Output/", " ",sp.list[m], " RoadEffect.csv", sep=""))
road.plot<-ggplot()+
       geom_ribbon(data=road.dat, aes(x= lakelevel, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=lake.dat, aes(y=mu, x=lakelevel))+
    geom_point(data=sp.dat, aes(y=maxocc, x=lakelevel))+
    theme_classic()+
    xlab("Variation in Road")+
    ylab("Max Count")

forest.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntForestEffect.csv", sep=""))
forest.plot<-ggplot()+
       geom_ribbon(data=forest.dat, aes(x= lakelevel, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=lake.dat, aes(y=mu, x=lakelevel))+
    geom_point(data=sp.dat, aes(y=maxocc, x=lakelevel))+
    theme_classic()+
    xlab("Variation in Forest")+
    ylab("Max Count")

##HERE##
forest.dat<-read.csv(paste("Output/", " ",sp.list[m], " PcntForestEffect.csv", sep=""))
forest.plot<-ggplot()+
       geom_ribbon(data=forest.dat, aes(x= lakelevel, ymin=LCI, ymax=UCI), fill="grey80")+
     geom_line(data=lake.dat, aes(y=mu, x=lakelevel))+
    geom_point(data=sp.dat, aes(y=maxocc, x=lakelevel))+
    theme_classic()+
    xlab("Variation in Forest")+
    ylab("Max Count")


plot4<-plot_grid(agri.plot, dev.plot, wet.plot, lake.plot)

#print plot and then turn device off
pdf(paste(out.dir, sp.list[m], ".PredictedEffectPlot.pdf", sep=""))
  try(print(plot4, silent=T))
while(!is.null(dev.list())) dev.off()
  
}#end sp loop

```

#Basin area weighted composite trend outputs

```{r}

all_comb<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 10, byrow = FALSE, dimnames = NULL))
names(all_comb) <- c("taxa_code","med_alph", "lcl_alph", "ucl_alph", "iw_alph", "med_tau", "lcl_tau", "ucl_tau", "iw_tau", "n")
write.table(all_comb, file = paste(out.dir, "CompositeOutputs.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

postsum<-read.csv("Output/ PosteriorSummary.csv")
area<-read.csv("Data/BasinArea.csv")

postsum<-merge(postsum, area, by.x="basin", by.y="lake")

sp.list<-unique(postsum$taxa_code)

for(m in 1:length(sp.list)) {
  
#m<-1  #for testing   

  com.data<-NULL
  com.data <- filter(postsum, taxa_code == sp.list[m]) %>%
      droplevels()
  
#sum to one area (i.e., percent)
com.data<-com.data %>% mutate(totbasin=sum(basinAreaKm2), perbasin=basinAreaKm2/totbasin) %>% select(-watershedAreaKm2, -basinAreaKm2, -totbasin)

##Weighted average https://www.wikihow.com/Calculate-Weighted-Average

#pool estimates
tau_prov <- com.data %>%
    summarise(med_tau=weighted.mean(tau, perbasin), lcl_tau=weighted.mean(tau_ll, perbasin),
            ucl_tau=weighted.mean(tau_ul, perbasin), iw_tau=ucl_tau-lcl_tau,
            n=n_distinct(basin))
tau_prov$taxa_code <- sp.list[m]


alph_prov <- com.data %>%
    summarise(med_alph=weighted.mean(alph, perbasin), lcl_alph=weighted.mean(alph_ll, perbasin),
            ucl_alph=weighted.mean(alph_ul, perbasin), iw_alph=ucl_alph-lcl_alph)
alph_prov$taxa_code <- sp.list[m]

com.data<-merge(alph_prov, tau_prov, by="taxa_code")

write.table(com.data, paste(out.dir, "CompositeOutputs.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)
  

} #end species list

```

#Basin area weighted composite trend outputs 

```{r}

all_comb<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 10, byrow = FALSE, dimnames = NULL))
names(all_comb) <- c("taxa_code","med_alph", "lcl_alph", "ucl_alph", "iw_alph", "med_tau", "lcl_tau", "ucl_tau", "iw_tau", "n")
write.table(all_comb, file = paste(out.dir, "CompositeOutputs.csv"), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

postsum<-read.csv("Output/ PosteriorSummary.csv")
area<-read.csv("Data/BasinArea.csv")

postsum<-merge(postsum, area, by.x="basin", by.y="lake")

sp.list<-unique(postsum$taxa_code)

for(m in 1:length(sp.list)) {
  
#m<-1  #for testing   

  com.data<-NULL
  com.data <- filter(postsum, taxa_code == sp.list[m]) %>%
      droplevels()
  
#sum to one area (i.e., percent)
com.data<-com.data %>% mutate(totbasin=sum(basinAreaKm2), perbasin=basinAreaKm2/totbasin) %>% select(-watershedAreaKm2, -basinAreaKm2, -totbasin)

##Weighted average https://www.wikihow.com/Calculate-Weighted-Average

#pool estimates
tau_prov <- com.data %>%
    summarise(med_tau=weighted.mean(tau, perbasin), lcl_tau=weighted.mean(tau_ll, perbasin),
            ucl_tau=weighted.mean(tau_ul, perbasin), iw_tau=ucl_tau-lcl_tau,
            n=n_distinct(basin))
tau_prov$taxa_code <- sp.list[m]


alph_prov <- com.data %>%
    summarise(med_alph=weighted.mean(alph, perbasin), lcl_alph=weighted.mean(alph_ll, perbasin),
            ucl_alph=weighted.mean(alph_ul, perbasin), iw_alph=ucl_alph-lcl_alph)
alph_prov$taxa_code <- sp.list[m]

com.data<-merge(alph_prov, tau_prov, by="taxa_code")

write.table(com.data, paste(out.dir, "CompositeOutputs.csv"), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)
  

} #end species list

```


